{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes module\n",
    "\n",
    "### Variational inference\n",
    "\n",
    "Given a score $S$, a parametric family of distributions $(\\nu_{\\theta})_{\\theta \\in \\Theta}$ and a prior distribution $\\pi$, we consider the variational problem\n",
    "\n",
    "$$\\hat{\\theta} = \\arg\\inf \\nu_{\\theta}[S] + \\lambda * KL(\\nu_{\\theta}, \\pi).$$\n",
    "\n",
    "The function variational_inference is designed to tackle such problems in the setting where $\\pi =  \\nu_{\\theta_0}$. This is in order to benefit from potential closed form expressions when computing the Kullback--Leibler divergence and its derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from aduq.bayes import iter_prior, variational_inference, iter_prior_vi\n",
    "from aduq.proba import GaussianMap, TensorizedGaussianMap\n",
    "\n",
    "# For plotting purposes\n",
    "from math import pi\n",
    "angles = np.linspace(0, 2 * pi, 1000)\n",
    "circle = np.array([np.cos(angles), np.sin(angles)])\n",
    "\n",
    "def half_cov(cov):\n",
    "    vals, vects = np.linalg.eigh(cov)\n",
    "    return (np.sqrt(vals) * vects) @ vects.T\n",
    "\n",
    "def repr_gauss(mean, cov, rad =1.0):\n",
    "    loc_circle = circle.copy()\n",
    "    return mean + rad * (half_cov(cov) @ loc_circle).T\n",
    "\n",
    "# Toy score function\n",
    "def score(x):\n",
    "    return (x @ np.array([0, 1]) - 1) ** 2 + 10 * (x @ np.array([1, -1])) **2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define the space of probability distributions on which we wish to optimize. Here we consider a score defined on a two dimensional space, and therefore use gaussian distributions on $\\mathbb{R}^2$. The prior will be the standard distribution\n",
    "\n",
    "It is normal behavior that the optimisation procedure raises some ProbaBadGrad warnings.These indicate that a problematic gradient estimation was rejected as it damaged significantly the score. No need to worry about those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss_map = GaussianMap(2)\n",
    "\n",
    "# We define the prior as the reference gaussian distribution, i.e. N(0,Id)\n",
    "prior_param = gauss_map.ref_param\n",
    "\n",
    "# To solve the variational inference problem, we use the variational_inference function.\n",
    "opt_res = variational_inference(\n",
    "    score, gauss_map,\n",
    "    prior_param=prior_param,\n",
    "    temperature=.1, # the lambda term in the variational inference problem\n",
    "    per_step=160,\n",
    "    VI_method='corr_weights',\n",
    "    gen_decay=np.log(1.1),\n",
    "    k = 160 * 20,\n",
    "    parallel=False,\n",
    "    vectorized=True,\n",
    "    print_rec=100, chain_length=501,\n",
    "    refuse_conf=.95,\n",
    "    momentum=.95, eta=0.6, silent=False)\n",
    "\n",
    "# It is normal behavior that the optimisation procedure raises some ProbaBadGrad warnings.\n",
    "# These indicate that a problematic gradient estimation was rejected as it damaged significantly\n",
    "# the score. No need to worry about those.\n",
    "\n",
    "# We can access the parameter describing the posterior through the opti_param attribute\n",
    "post_param = opt_res.opti_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The optimisation start by modification of the covariance \n",
    "\n",
    "for i, param in enumerate(opt_res.hist_param[:13:2]):\n",
    "    if i % 1 == 0:\n",
    "        distr = gauss_map(param)\n",
    "        distr_repr = repr_gauss(distr.means, distr.cov)\n",
    "        plt.plot(distr_repr[:,0], distr_repr[:,1], color='black', linewidth=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The distribution then shifts towards the correct mean value\n",
    "for i, param in enumerate(opt_res.hist_param[20:160:20]):\n",
    "    if i % 1 == 0:\n",
    "        distr = gauss_map(param)\n",
    "        distr_repr = repr_gauss(distr.means, distr.cov)\n",
    "        plt.plot(distr_repr[:,0], distr_repr[:,1], color='black', linewidth=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The last steps then adjust the distribution\n",
    "for i, param in enumerate(opt_res.hist_param[150:200:2]):\n",
    "    if i % 1 == 0:\n",
    "        distr = gauss_map(param)\n",
    "        distr_repr = repr_gauss(distr.means, distr.cov)\n",
    "        plt.plot(distr_repr[:,0], distr_repr[:,1], color='black', linewidth=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The evolution of the VI score can also be tracked:\n",
    "plt.plot(opt_res.hist_score)\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the hood, variational_inference can redirect to two routines (VI_method argument): either \"corr_weights\" or \"KNN\". The name refers to the method used in order to make most use of the evaluations to the score function.\n",
    "\n",
    "The 'variational_inference' function was designed for situations where evaluating the 'score' is rather expensive. It is still, however, an accelarated gradient descent algorithm. The change is that the gradient's expression involves an expectation with respect to the current distribution. The na√Øve approach consisting in sampling iid samples from the current distribution to obtain an unbiased estimation of the expectation is improved upon by recycling previous samples. These are generated from distributions similar to the current one, if small optimization steps are done ('eta' parameter is small).\n",
    "\n",
    "As it is not possible to use these samples directly, two procedures are proposed. \"corr_weights\" consists in giving each sample a weight to adjust for the difference of probability for it being drawn between the current and previous distributions. \"KNN\" consists in constructing a surrogate score using a K-Nearest neighbor algorithm, then using this surrogate on a large number of samples to compute the derivative.\n",
    "\n",
    "The number of samples used all in all when evaluating the derivative is controlled by the argument 'k'. By default it is None, amounting to all samples being used.\n",
    "\n",
    "For \"corr_weights\", it is possible and advisable to set the 'gen_decay' parameter higher than 0 (default value). The 'gen_decay' parameter gives a decreasing weights to older generations when computing the derivative. While generations just before tend to be close to the current one, older ones would no longer be representative, and could have a negative impact when computing the derivative. The higher 'gen_decay', the lower will be the influence of older generation (exponentially decreasing weights of $\\exp(-t \\times gen\\_decay)$ are used).\n",
    "\n",
    "For \"KNN\", the number of neighbors used by the K-nearest neighbors algorithm is NOT controlled by the argument 'k', but by \"n_neighbors\". As stated above, \"k\" controls the number of samples used. By default, \"n_neighbors\" is 5.\n",
    "\n",
    "\n",
    "The 'corr_weights' method has the edge in most cases. For instance, 'KNN' by design does not like situations where the Hessian near the minima has eigenvalues of different magnitudes, which is the case for the Rosenbrock function tested here. This could be improved upon by learning the distance used in 'KNN', or by training different surrogates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For comparison, variational_inference with KNN method\n",
    "\n",
    "opt_res = variational_inference(\n",
    "    score, gauss_map,\n",
    "    prior_param=prior_param,\n",
    "    temperature=.1,\n",
    "    per_step=160,\n",
    "    VI_method='KNN',\n",
    "    k = None,\n",
    "    parallel=False, print_rec=20, chain_length=600,\n",
    "    vectorized=True,\n",
    "    momentum=.99, eta=0.1, silent=True)\n",
    "\n",
    "end_distr = gauss_map(opt_res.opti_param)\n",
    "\n",
    "print(f\"Mean score of estimated posterior: {end_distr.integrate(score, n_sample = 1000)}\")\n",
    "\n",
    "# The evolution of the VI score can also be tracked:\n",
    "plt.plot(opt_res.hist_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iter prior procedure\n",
    "\n",
    "The iterated prior procedure is not a Bayesian technique at all. It is actually an optimisation routine, using a Bayesian flavored technique.\n",
    "\n",
    "The goal is minimizing $S(x)$, a score function. In order to do that, parameters are drawn from a distribution. The distribution for the next generation is then obtained by centering around the best parameter found so far, and by using the top parameters found so far to construct the covariance. Each dimension of the parameter is drawn independantly from a gaussian distribution, so that the covariance is diagonal and can be defined by using the empirical standard deviations of the top parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The initial prior_param is a parameter for the TensorizedGaussianMap.\n",
    "ini_prior = np.zeros((2,2))\n",
    "ini_prior[1] = 1.0\n",
    "\n",
    "opt_res = iter_prior(score, ini_prior_param = ini_prior, gen_per_step=800, chain_length=50, keep=100, frac_sparse=0.0, parallel=False)\n",
    "\n",
    "# The opti_param attribute of opt_res gives a distribution and NOT a parameter\n",
    "opti_distr_param = opt_res.opti_param\n",
    "\n",
    "# The optimal parameter can still be found:\n",
    "opti_param = opt_res.full_sample[0]\n",
    "print(opti_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The technique used in iter prior can still be useful in the context of variational inference, in order to construct quickly a good initial distribution. The function iter_prior_vi is designed precisely for that purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_res = iter_prior_vi(\n",
    "    score,\n",
    "    prior_param = ini_prior, temperature=0.1, gen_per_step=800, chain_length=50, keep=100, frac_sparse=0.0,\n",
    "    parallel=False, vectorized=True)\n",
    "\n",
    "# The opti_param attribute of opt_res gives a distribution and NOT a parameter\n",
    "opti_distr_param = opt_res.opti_param\n",
    "\n",
    "start_post = np.zeros((3,2))\n",
    "\n",
    "start_post[0] = opti_distr_param[0]\n",
    "start_post[1:] = np.diag(opti_distr_param[1])\n",
    "\n",
    "opt_res = variational_inference(\n",
    "    score, gauss_map,\n",
    "    prior_param=prior_param,\n",
    "    post_param=start_post,\n",
    "    temperature=.1,\n",
    "    per_step=160,\n",
    "    VI_method='corr_weights',\n",
    "    gen_decay=np.log(1.2),\n",
    "    k = 160 * 20,\n",
    "    parallel=False,\n",
    "    vectorized=True,\n",
    "    print_rec=2, chain_length=50,\n",
    "    refuse_conf=.95,\n",
    "    momentum=.95, eta=0.1, silent=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(opt_res.hist_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uniform priors - Gaussian computations\n",
    "\n",
    "The proba module offers a class of distributions on the hypercube benefitting from Gaussian like interpretation when the distribution are sufficiently concentrated and exact computations for KL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aduq.proba.gauss import GaussHypercubeMap\n",
    "\n",
    "dim = 2\n",
    "\n",
    "# Toy score function\n",
    "def score(x):\n",
    "    return (x @ np.array([1.0, 0.0], dtype=np.float64) - .6) ** 2 + 20 * (x @ np.array([1.0, -1.0], dtype=np.float64)) **2\n",
    "\n",
    "pmap = GaussHypercubeMap(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_res = variational_inference(\n",
    "    score, pmap,\n",
    "    temperature=.1, # the lambda term in the variational inference problem\n",
    "    per_step=160,\n",
    "    VI_method='corr_weights',\n",
    "    gen_decay=np.log(1.3),\n",
    "    k = 160 * 30,\n",
    "    parallel=False,\n",
    "    vectorized=True,\n",
    "    print_rec=10,\n",
    "    chain_length=201,\n",
    "    refuse_conf=.95,\n",
    "    momentum=.95, eta=0.4, silent=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The posterior can adapt to scores with strong identifiability issues such as Rosenbrock, since the probabilities can exhibit strong correlation structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "proba = pmap(opt_res.opti_param)\n",
    "# The log density of the function can be accessed through log_dens\n",
    "x_axis_labels = np.linspace(10**-4,1 - 10** -4, 121)\n",
    "y_axis_labels = np.linspace(10**-4,1- 10 ** -4, 121)\n",
    "\n",
    "values = np.array(np.meshgrid(y_axis_labels, x_axis_labels)).T\n",
    "z = proba.dens(values)\n",
    "\n",
    "sns.heatmap(z, xticklabels=x_axis_labels, yticklabels=y_axis_labels)\n",
    "plt.title(\"Posterior distribution\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(opt_res.hist_score, label=\"Evolution of the VI score\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
