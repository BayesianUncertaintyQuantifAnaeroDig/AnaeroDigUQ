{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ff5e609",
   "metadata": {},
   "source": [
    "# Proba module\n",
    "\n",
    "The proba module is designed to encode continuous probability distributions on array like objects and simplify manipulation of probability distributions and optimisation tasks on functionals of probability distributions.\n",
    "\n",
    "The module is constructed around two main classes\n",
    "\n",
    "- Proba, the probability distribution object.\n",
    "- ProbabMap, parametric families of probability distributions.\n",
    "\n",
    "Potential improvements:\n",
    "- Distribution class, for general measure (potentially signed, etc) and unnormalised posteriors\n",
    "- Unify output shape checks.\n",
    "\n",
    "The current demo's goal is to showcase the use of both classes as well as show how custom subclasses can be constructed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d320abe8",
   "metadata": {},
   "source": [
    "# Proba class\n",
    "\n",
    "## What is a Proba object?\n",
    "The Proba class describes a probability distribution. A probability distribution is defined through\n",
    "- a generator function, which generates samples.\n",
    "- a log density function (the density being defined wrt some knwon reference distribution).\n",
    "\n",
    "\n",
    "__No checks are performed to ensure that the generator and log density work well together! This is the user's responsibility.__\n",
    "\n",
    "For convenience, the shape of the sample can be passed to the distribution when constructing. If it is not passed, it is infered from the generator function. The generator should generate samples as an iterable of ArrayLike object with shape matching sample_shape.\n",
    "\n",
    "We give exemples of simple manipulation of a Proba object using a Gaussian distribution (subclassed from Proba, methods are modified to increase efficiency)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd703e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Main imports\n",
    "import numpy as np\n",
    "import aduq.proba as proba\n",
    "\n",
    "# Visualisation imports\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# We showcase a simple Proba object describing a Gaussian distribution\n",
    "# Here we use the subclass Gaussian to avoid re-implementation of the log_dens function\n",
    "mean = np.array([2,0])\n",
    "cov = .01 * np.array([[2,.4],[.4,2]])\n",
    "\n",
    "gauss = proba.Gaussian(mean, cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc295aa9",
   "metadata": {},
   "source": [
    "A Proba object is defined through 2 mechanisms: a sampler, and its log density function.\n",
    "\n",
    "### The gen function\n",
    "Can be called through the 'gen' attribute or simply by calling the proba object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5723c8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = gauss(10000) # Generate a sample\n",
    "sample = gauss.gen(10000) # Equivalent code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6826aab7",
   "metadata": {},
   "source": [
    "### The log_dens function\n",
    "\n",
    "The log_dens function is the second main attribute of a proba object. As its name suggests, it computes the log density of the distribution at given points. The reference distribution wrt to which the log-density is computed is assumed known. A good practice consists in using Lebesgue measure.\n",
    "\n",
    "The log_dens is a 'vectorized' function. Assuming the probability is defined on arrays of shape (d1, ..., dn), the log-density takes as input arrays of shape (n1, ..., nk, d1, ..., dk) and outputs an array of shape (n1, .., nk)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0b8b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the marginal distribution on the first dimension - this can only be done for Gaussian distributions\n",
    "sub_gauss = gauss.marginalize([0])\n",
    "\n",
    "# Plot the histogram of marginal sample on first dimension\n",
    "plt.hist(sample[:, 0], 40, density=True, label=\"Histogram\")\n",
    "\n",
    "# Plot the density of the marginal distribution on first dimension\n",
    "xs = np.linspace(1, 3, 100).reshape((100, 1))\n",
    "ys = sub_gauss.log_dens(xs)\n",
    "plt.plot(xs, np.exp(ys), label='Density')\n",
    "plt.legend()\n",
    "plt.title(\"Distribution of the marginal distribution of a multivariate gaussian\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac67b01",
   "metadata": {},
   "source": [
    "There are no checks during construction that the log-density and generating mechanism work coherently. That is the job of the person implementing the instance. However, if the distribution is well implemented, the transforms encoded in the package should preserve that property.\n",
    "\n",
    "\n",
    "### Further attributes\n",
    "\n",
    "A generic proba object contains a further public attribute\n",
    "- sample_shape, the shape of a single sample point after conversion to np.ndarray\n",
    "\n",
    "Two private attributes are also logged:\n",
    "- _sample_size, the number of dimensions of a single sample point (short of np.prod(self.sample_shape))\n",
    "- _np_out, states whether the output of the gen function are encoded as np.ndarray of shape (n,) + sample_shape\n",
    "\n",
    "If _np_out is True, some of the implementations are modified to make use of this fact. Good implementation would be to implement a proba instance such that _np_out is True if possible. Since there are some cases where this might not be wanted, it is not enforced by default.\n",
    "\n",
    "Inherited class might have other attributes, force instance used to pre compute some values. For instance, the Gaussian class has attribute inv_cov, avoiding having to repeatedly compute the inverse when computing the log density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b28cb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sample_shape attribute inform on the shape of generated samples\n",
    "assert gauss(10).shape == ( (10, ) + gauss.sample_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9663e81d",
   "metadata": {},
   "source": [
    "## Methods - integrals and metrics\n",
    "A Proba object can be used to estimate expected values and pseudo distance between probabilities (KL divergence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c2e328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integration can be done for functions outputing np.ndarray of any shape of outputing floats/ints,\n",
    "# and potentially using other arguments.\n",
    "def fun(x, c ):\n",
    "    return np.sum(x **2) < c\n",
    "\n",
    "c = np.linspace(2, 6, 100)\n",
    "\n",
    "plt.plot(c, gauss.integrate(fun, n_sample = 10000, c = c))\n",
    "\n",
    "plt.xlabel(\"C\")\n",
    "plt.ylabel(r\"$\\mathbb{P}[\\mid X\\mid^2 \\leq C]$\")\n",
    "plt.show()\n",
    "\n",
    "gauss2 =  proba.Gaussian(np.array([-3, 2]), .1 * cov)\n",
    "print(f\"KL between two gaussians: {gauss.kl(gauss2)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7e0b98",
   "metadata": {},
   "source": [
    "## Proba instance from scratch\n",
    "To construct a Proba instance, one needs two ingredients:\n",
    "- a way to generate samples from the distribution\n",
    "- the log density function with respect to a known distribution.\n",
    "\n",
    "We construct a Proba object describing a uniform distribution from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072f7ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider you want to define the distribution U[0,1]\n",
    "\n",
    "def gen(n):\n",
    "    \"\"\" \n",
    "    numpy is great!\n",
    "    \"\"\"\n",
    "    # It is good practice to consider that the output is a np.ndarray, not a float \n",
    "    return np.random.uniform(0,1, (n,1)) \n",
    "\n",
    "def log_dens(xs):\n",
    "    ''' The log density should be vectorized as much as possible'''\n",
    "    ys = np.array(xs)\n",
    "    ys = ys.reshape(ys.shape[:-1])\n",
    "    below_0 = ys < 0\n",
    "    above_1 = ys > 1\n",
    "    out = np.zeros(ys.shape)\n",
    "    out[below_0] = - np.inf\n",
    "    out[above_1] = - np.inf\n",
    "    return out\n",
    "\n",
    "unif = proba.Proba(gen, log_dens, sample_shape = (1,))\n",
    "\n",
    "xs = np.array(np.linspace(-.5, 1.5, 400).reshape((400,1)))\n",
    "ys = unif.dens(xs)\n",
    "plt.plot(xs,ys)\n",
    "plt.hist(unif(10**4), 40, density=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a405f8",
   "metadata": {},
   "source": [
    "Notice that we had to define the density and the generator independantly. As stated before, no checks are conducted during construction, and it is the user's responsability to pass coherent arguments.\n",
    "\n",
    "## Inherit from Proba\n",
    "For specific forms of distribution, one might want to subclass Proba.\n",
    "For instance,\n",
    "- Linear transforms of uniforms resulting in uniform distributions. We want to rewrite the lin_transform method to keep that information.\n",
    "- Moreover, one knows that to estimate integrals of smooth functionals on low dimensional space, Monte Carlo is less efficient than other sampling methods. We want to write a custom integration routines using QMC sampling.\n",
    "\n",
    "This is done in the followig manner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b74d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new distribution\n",
    "\n",
    "class Unif(proba.Proba):\n",
    "    \"\"\" An uniform distribution class \"\"\"\n",
    "    def __init__(self, x_min, x_max):\n",
    "\n",
    "        if x_max < x_min:\n",
    "            raise Exception(f\"{x_min} should be < than {x_max}\")\n",
    "\n",
    "        self.x_min = x_min\n",
    "        self.x_max = x_max\n",
    "\n",
    "        def gen(n):\n",
    "            return np.random.uniform(self.x_min, self.x_max, (n,1))\n",
    "\n",
    "        dens_val = - np.log(self.x_max - self.x_min)\n",
    "\n",
    "        def log_dens(xs):\n",
    "            xs = np.array(xs)\n",
    "            ys = xs.reshape(xs.shape[:-1])\n",
    "            below_0 = ys < self.x_min\n",
    "            above_1 = ys > self.x_max\n",
    "            out = np.full(ys.shape, dens_val)\n",
    "            out[below_0] = - np.inf\n",
    "            out[above_1] = - np.inf\n",
    "            return out\n",
    "\n",
    "                \n",
    "        super().__init__(gen, log_dens, (1,))\n",
    "        \n",
    "    def integrate(\n",
    "        self, \n",
    "        func,\n",
    "        n_sample: int = 100,\n",
    "        parallel: bool = False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        # Your custom trick for estimating integrals: evenly spaced samples\n",
    "        sample = np.linspace(self.x_min, self.x_max, n_sample).reshape((n_sample,1))\n",
    "\n",
    "        # One should properly give a possibility for parallelization, we'll skip it here\n",
    "        vals = np.array([func(x, **kwargs) for x in sample])\n",
    "\n",
    "        if len(vals.shape) > 1: # Check multidimensional output.\n",
    "            return np.apply_along_axis(np.mean, 0, np.array(vals))\n",
    "\n",
    "        return np.mean(vals)\n",
    "\n",
    "\n",
    "    def lin_transform(self, mat: np.ndarray, shift = 0.0):\n",
    "        # We follow the guidelines for lin_transform\n",
    "        mat_val = mat[0, 0]\n",
    "        x_min = self.x_min * mat_val + shift\n",
    "        x_max = self.x_max * mat_val + shift\n",
    "        x_min, x_max = min(x_min, x_max), max(x_min, x_max)\n",
    "        return Unif(x_min, x_max)\n",
    "\n",
    "std_unif = Unif(0,1)\n",
    "\n",
    "def func(x):\n",
    "    return x[0]**3\n",
    "\n",
    "ns = np.arange(10, 100, 2)\n",
    "plt.plot(ns, [std_unif.integrate(func, n) for n in ns], label='QMC')\n",
    "plt.plot(ns, [unif.integrate(func, n) for n in ns], label='MC')\n",
    "plt.plot(ns, [0.25 for n in ns], \"--\", c=\"black\")\n",
    "\n",
    "plt.xlabel(\"n\")\n",
    "plt.legend()\n",
    "plt.title(\"Advantage of custom Unif subclass! QMC thrashes MC\")\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "# Check that the lin_transform is correctly implemented\n",
    "plt.hist(std_unif(100000), 30, density=True, label=\"U(0,1)\")\n",
    "\n",
    "mod_unif = std_unif.lin_transform(mat=np.array([[2.0]]), shift=-0.5)\n",
    "plt.hist(mod_unif(100000), 30, density=True, label=\"2 * U(0,1) - .5\")\n",
    "plt.legend()\n",
    "\n",
    "X = np.linspace(-.75, 1.75, 1000).reshape((1000,1))\n",
    "Y1 = std_unif.dens(X)\n",
    "Y2 = mod_unif.dens(X)\n",
    "plt.plot(X,Y1)\n",
    "plt.plot(X,Y2)\n",
    "plt.title(\"The distribution of the linear transform is as expected\")\n",
    "\n",
    "plt.show()\n",
    "print(f\"Class of linearly transformed Unif: {type(mod_unif)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4457193f",
   "metadata": {},
   "source": [
    "## Methods - Transformations\n",
    "\n",
    "Since it inherits from Proba, the new class benefits from the already implemented transformation methods which have not been re-implemented.\n",
    "A useful transform consists in reshaping the output. For instance, we can modify our distribution such that samples are now outputted as np.ndarray of shape (1,1).\n",
    "\n",
    "More general than lin_transform, the transform transforms the distribution of X to the distribution of f(X) if f is bijective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c0959f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test reshape\n",
    "ini_shape = std_unif.sample_shape\n",
    "c = std_unif.reshape(ini_shape+(1,))\n",
    "print(c(100).shape)\n",
    "\n",
    "# Check that it is still the same good old unif distrib\n",
    "plt.hist(c(100000).flatten(), 100, density=True)\n",
    "plt.title(\"It is still a good old uniform!\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fdf571",
   "metadata": {},
   "source": [
    "More general than lin_transform, the transform transforms the distribution of X to the distribution of f(X) if f is bijective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7ee990",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = np.exp\n",
    "inv_transform = np.log \n",
    "\n",
    "def der_transform(xs):\n",
    "    # Here we are considering distributions on outputs of shape (1,) and a transform from (1,) to (1,)\n",
    "    # The rule is that the der_transform take arrays of shape (n, 1,) and outputs an array of shape (n, 1, 1)\n",
    "    # (the general rule is (shape_n, shape_in)-> (shape_n, shape_in, shape_out), the derivative being vectorized\n",
    "\n",
    "    out =  np.exp(xs)\n",
    "    return out.reshape( xs.shape[:-1] + (1,1))\n",
    "\n",
    "new_distr = std_unif.transform(transform=transform, inv_transform=inv_transform, der_transform=der_transform)\n",
    "xs = np.linspace(.5, 3, 1000).reshape((1000, 1))\n",
    "\n",
    "plt.hist(np.array(new_distr(10000))[:,0], 40, density=True, label='np.exp(unif(n))')\n",
    "plt.hist(np.array(np.exp(std_unif(10000)))[:,0], 40, density=True, label='unif.transform(exp)(n)' )\n",
    "plt.plot(xs, new_distr.dens(xs), label='unif.transform(exp).log_dens')\n",
    "plt.title(\"Distribution of exp(U[0,1])\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84c8847",
   "metadata": {},
   "source": [
    "Apart from transformation, Proba objects can be assembled through tensorisation or mixtures. They can be constructed using a Kernel and a sample through convolution.\n",
    "\n",
    "For Gaussian, non bijective linear transforms are allowed as long as the transform is sujerective. This behavior is not allowed in the general case, as the log density computation requires integrating."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669bc1b0",
   "metadata": {},
   "source": [
    "Another standard operation consists in defining a mixture of distributions. By defaults, distributions are given equal weights though it is possible to specify the weights through the weights argument. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10531427",
   "metadata": {},
   "outputs": [],
   "source": [
    "unif_mixture = proba.mixture(std_unif, mod_unif, mod_unif.shift(.2), weights=[.3, .5, .2])\n",
    "Y3 = np.exp(unif_mixture.log_dens(X))\n",
    "\n",
    "plt.hist(np.array(unif_mixture(40000)), 30, density=True)\n",
    "plt.plot(X, Y3)\n",
    "plt.title(\"Uniform mixture\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee52cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showcasing mixture in multidimension\n",
    "gauss_mixt = proba.mixture(gauss, gauss2)\n",
    "\n",
    "# A distribution stores essentially two attributes: Sampling and density\n",
    "# To sample, simply call the distribution\n",
    "sample = np.array(gauss_mixt(100))\n",
    "\n",
    "plt.plot(sample[:,0], sample[:, 1], '.')\n",
    "plt.title(\"A sample from a 2 dimensional gaussian distribution\")\n",
    "plt.clf()\n",
    "\n",
    "# The log density of the function can be accessed through log_dens\n",
    "x_axis_labels = np.linspace(-5,5, 81)\n",
    "y_axis_labels = np.linspace(-5,5, 81)\n",
    "values = np.array(np.meshgrid(y_axis_labels, x_axis_labels)).T # y, x necessary. Note y axis will be plotted from up (-5) to bottom (5).\n",
    "values.reshape(values.shape + (1,))\n",
    "\n",
    "z = gauss_mixt.log_dens(values)\n",
    "\n",
    "sns.heatmap(z, xticklabels=x_axis_labels, yticklabels=y_axis_labels)\n",
    "plt.xticks([]) # remove ticks\n",
    "plt.yticks([]) # remove ticks\n",
    "\n",
    "plt.title(\"Log-Density of a Gaussian mixture\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385f68fa",
   "metadata": {},
   "source": [
    "It is important to note that, depending on how the class is coded, there could be side effect when modifying the initial distribution. Here for instance, the distribution A depends on two parameters which can be adjusted afterwards (x_min, x_max). If they are changed, then A changes and the distribution B = 2 * A - .5 is changed in such a way that the relationship is maintained. This can be either useful or a bother, so the subclass should be implemented in whichever way is satisfactory (for side effect, code generator/log_density, and methods using specific attributes used to generate the distribution, i.e. store them as self.x and code with self.x, for no side effect, store them as self._x and code with x)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5293efc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"b = a + 10 is defined through custom lin_transform (reimplemented in Unif class)\")\n",
    "b = std_unif.lin_transform(mat = np.array([[1]]), shift=10) \n",
    "std_unif.x_max = 1.0\n",
    "print(f\"Maximum value of b when a max={std_unif.x_max}: {np.ceil(np.max(b(1000)))}\")\n",
    "\n",
    "std_unif.x_max = 10.0\n",
    "print(f\"Maximum value of b when a max={std_unif.x_max}: {np.ceil(np.max(b(1000)))}\")\n",
    "\n",
    "std_unif.x_max = 1.0\n",
    "\n",
    "print(\"\\nb = a + 10 is defined through default shift (not reimplemented in Unif class)\")\n",
    "b = std_unif.shift(10)\n",
    "std_unif.x_max = 1.0\n",
    "print(f\"Maximum value of b when a max={std_unif.x_max}: {np.ceil(np.max(b(1000)))}\")\n",
    "\n",
    "std_unif.x_max = 10.0\n",
    "print(f\"Maximum value of b when a max={std_unif.x_max}: {np.ceil(np.max(b(1000)))}\")\n",
    "\n",
    "std_unif.x_max = 1.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0597425",
   "metadata": {},
   "source": [
    "Other methods include computation of pseudo metrics on probablity spaces, the Kullback Leibler divergence or general f-divergences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c24547e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = proba.TensorizedGaussian(means= [1], devs=[2])\n",
    "n2 = proba.TensorizedGaussian(means= [2], devs=[3])\n",
    "print(\"n1 = N(1,2), n2 = N(2,3)\")\n",
    "print(\"kl(n1, n2):\", n1.kl(n2, n_sample=1000))\n",
    "print(\"kl(n2, n1):\", n2.kl(n1, n_sample=1000))\n",
    "print()\n",
    " \n",
    "print(\"kl(U[0,1], n1):\", std_unif.kl(n1, n_sample=1000))\n",
    "print(\"kl(n1, U[0,1]):\", n1.kl(std_unif, n_sample=1000))\n",
    "print()\n",
    "\n",
    "print(\"Hellinger(n1, n2):\", n1.f_div(n2, lambda x: x ** 2 /2 - .5, n_sample=10000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b8b9c4",
   "metadata": {},
   "source": [
    "A distribution object can also be constructed from a sample and a kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1469c26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = n2(500)\n",
    "res = proba.proba.from_sample(sample, Unif(0, 1).shift(-.5).contract(.1))\n",
    "\n",
    "plt.hist(n2(100000)[:, 0], 60, density=True)\n",
    "plt.hist(np.array(res(100000))[:,0], 60, density=True)\n",
    "\n",
    "plt.title(\"Reconstruction of a normal distribution through 500 samples and a uniform kernel\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1327c7",
   "metadata": {},
   "source": [
    "# ProbaMap\n",
    "\n",
    "A ProbaMap object is a parametric family of distributions defined on the same space. It is created through a map function (taking a ProbaParam, i.e. an array like object, and outputing a Proba), and a log_dens_der function outputing a closure.\n",
    "\n",
    "Basically, it is a way to construct Proba objects, with some additional information which are useful when optimising criterias of form\n",
    "\n",
    "$$\\mu \\rightarrow \\mathbb{E}_{X\\sim \\mu}[f(X)] + F(\\mu)$$\n",
    "\n",
    "### What is log_dens_der and why does it have to be coded as a function outputing a closure?\n",
    "\n",
    "log_dens_der is the derivative $\\nabla_{\\theta}\\log( \\frac{d\\pi_\\theta}{d\\pi^*}(x)))$, and as such is a function of both $x$ and $\\theta$. For families of density of form $C(\\theta)f(\\theta, x)$, where $C(\\theta)$ is a renormalisation constant, coding the derivative as a function of $\\theta$ outputing a function of $x$ enables to cache easily the derivative of $C$ with respect to $\\theta$ and reuse it for multiple evaluations of $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489cd4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-implemented ProbaMap objects are defined for Gaussian distributions\n",
    "dim = 2\n",
    "\n",
    "gmap = proba.GaussianMap(dim)\n",
    "par_shape = gmap.distr_param_shape\n",
    "distr_param = np.random.normal(0,1, par_shape)\n",
    "\n",
    "gaussian = gmap(distr_param) # Constructs a Proba object\n",
    "\n",
    "# Compute KL divergence from distr param\n",
    "ref_param = gmap.ref_param\n",
    "kl = gmap.kl(distr_param, ref_param)\n",
    "\n",
    "# Compute the derivative of the kl\n",
    "grad, kl2 = gmap.grad_kl(ref_param)(distr_param)\n",
    "assert kl2 == kl\n",
    "\n",
    "perturb = np.random.normal(0, 10** (-2), par_shape)\n",
    "assert np.abs((gmap.kl(distr_param + perturb, ref_param) - kl) / (np.sum(perturb * grad)) - 1) < 10 ** (3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bc2694",
   "metadata": {},
   "source": [
    "The ProbaMap class can be used to estimate gradients of integrals with respect to the probability distribution. Note that these estimates require large sample size to be valid, and as such, it is advised not to rely too heavily on this method for Gradient descent style algorithm if the function call is computationnally intensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360bf0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate the derivative of an integral\n",
    "def func(x):\n",
    "    return np.sum(x ** 2)\n",
    "print(\"Integral of squared norm\")\n",
    "grad_integral, integral = gmap.integrate_der(func, ref_param, n_sample = 10 ** 5)\n",
    "print(f\"\"\"The obtained delta was  {\n",
    "    gmap(ref_param + perturb).integrate(func, n_sample = 10 ** 5) - integral\n",
    "    }\"\"\")\n",
    "print(f\"\"\"The Delta estimated through the approximate gradient was {np.tensordot(perturb, grad_integral, ((0, 1), (0, 1)))}\\n\"\"\")\n",
    "\n",
    "# Estimate the derivative of multivariate integral (known)\n",
    "der, empir_means = gmap.integrate_der(lambda x:x, param=gmap.ref_param, n_sample = 10 ** 5)\n",
    "\n",
    "true_der = np.zeros(gmap.distr_param_shape + (2,))\n",
    "true_der[0, 0, 0] = 1.0\n",
    "true_der[0, 1, 1] = 1.0\n",
    "\n",
    "print(f\"Error in derivative computation for identity: {np.sum( (der - true_der)**2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9eb2c6",
   "metadata": {},
   "source": [
    "The parametrisation used by a ProbaMap object can be modified using the reparametrize method. This is notably useful for ExponentialFamily, which are defined using their natural parametrisation. We give a trivial exemple of reparametrize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8107f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(x):\n",
    "    return 2 * x\n",
    "\n",
    "def inverse(x):\n",
    "    return x / 2\n",
    "\n",
    "def der_transform(x):\n",
    "    \"\"\" For reparametrize, it is not necessary to use properly vectorized implementation \"\"\"\n",
    "    return 2 * np.eye(np.size(ref_param)).reshape(ref_param.shape + ref_param.shape)\n",
    "\n",
    "n_gmap = gmap.reparametrize(transform, der_transform, inverse, inherit_methods=True)\n",
    "\n",
    "import time\n",
    "# The new parametrisation can be used as before\n",
    "print(n_gmap.integrate_der(lambda x:x, param=n_gmap.ref_param, n_sample = 10 ** 4))\n",
    "\n",
    "# This is no longer a GaussianMap object but a general ProbaMap object\n",
    "print(f\"The resulting map is of class: {type(n_gmap )}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7faefbda",
   "metadata": {},
   "source": [
    "The key feature of reparametrize is method inheritance. Setting inherit_methods=True, the resulting map benefits from previously implemented methods for kl and its derivative. This is of particular interest whenever the kl expression can be written in closed form, which is the case for Exponential families."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc8957d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# But due to inherit_methods, it benefits from the efficient and exact kl/grad_kl\n",
    "# computation of the GaussianMap object!\n",
    "par1, par2 = np.random.normal(0, 1, (2,) + n_gmap.distr_param_shape)\n",
    "tic = time.time()\n",
    "n_gmap.grad_kl(par1)(par2)\n",
    "tac = time.time()\n",
    "print(f\"Using inherit_methods=True: {tac - tic} s. Computation is exact\")\n",
    "\n",
    "# For comparison\n",
    "n_gmap2 = gmap.reparametrize(transform, der_transform, inverse, inherit_methods=False)\n",
    "tic = time.time()\n",
    "n_gmap2.grad_kl(par1)(par2, n_sample = 10**4)\n",
    "tac = time.time()\n",
    "print(f\"Using inherit_methods=False: {tac - tic} s. Computation is approximate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3833713d",
   "metadata": {},
   "source": [
    "## Constructing a ProbaMap object from scratch\n",
    "\n",
    "While some standard parametric distributions are already available, it can be necessary to construct a tailored made ProbaMap object. We show how this can be done, taking as exemple uniform distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b56d379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom ProbaMap\n",
    "# We encourage defining maps with as little constraints as possible on the parameters passed.\n",
    "# For instance, Unif(a,b ) requires a < b.\n",
    "# To bypass that, we can consider Unif( min(a,b), max(a,b) )\n",
    "\n",
    "class UnifMap(proba.ProbaMap):\n",
    "    def __init__(self):\n",
    "\n",
    "        def map(x)->proba.Proba:\n",
    "            return Unif(np.min(x), np.max(x))\n",
    "\n",
    "        def log_dens_der(x):\n",
    "            \"\"\" \n",
    "            Compute the derivative\n",
    "            \"\"\"\n",
    "            x = np.array(x)\n",
    "            x_shape = x.shape\n",
    "            x = x.flatten()\n",
    "            \n",
    "            min_x = np.min(x)\n",
    "            max_x = np.max(x)\n",
    "\n",
    "            idx_min = (x == min_x) # indices achieving the minimum\n",
    "            idx_max = (x == max_x)\n",
    "            \n",
    "            def der(ys):\n",
    "                pre_shape = ys.shape[:-1]\n",
    "                ys = ys.flatten()\n",
    "                loc = np.zeros((np.prod(pre_shape),np.prod(x_shape)))\n",
    "                loc[ys == min_x] = -np.inf \n",
    "                loc[ys == max_x] = np.inf\n",
    "\n",
    "                between = (ys < min_x) & (ys > max_x)\n",
    "                loc[between, idx_max] = - 1/ (max_x - min_x)\n",
    "                loc[between, idx_min] = 1/ (max_x - min_x)\n",
    "\n",
    "                if ys == min_x:\n",
    "                    loc[idx_min] = - np.inf\n",
    "                if ys == max_x:\n",
    "                    loc[idx_max] = np.inf\n",
    "                if (y < min_x) & (y > max_x):\n",
    "                    # log_dens(y) = -np.log(x[idx_max] - x[idx_min])\n",
    "                    loc[idx_max] = - 1/ (max_x - min_x)\n",
    "                    loc[idx_min] = 1 / (max_x - min_x)\n",
    "\n",
    "                # Remaining case: log_dens(y) = 0 and nothing happens\n",
    "\n",
    "                return loc.reshape(pre_shape + x_shape)\n",
    "            \n",
    "            return der\n",
    "        super().__init__(map, log_dens_der, ref_param=[0,1], distr_param_shape=(2,), sample_shape=(1,))\n",
    "        \n",
    "    def kl(self, param1, param0, n_sample=0, parallel=False):\n",
    "        if (np.max(param1) == np.max(param0)) and (np.min(param0) == np.min(param1)):\n",
    "            return 0\n",
    "        else:\n",
    "            return np.inf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96aa993",
   "metadata": {},
   "source": [
    "Using reparametrize, we can change the parametrisation from min and max to the mean and half the width. That can easily be done using the reparametrize method inherited from ProbaMap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fb7d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "umap = UnifMap()\n",
    "\n",
    "# Reparametrization\n",
    "def transform(x):\n",
    "    return np.array([x[0]- x[1], x[0] + x[1]])\n",
    "\n",
    "def der_transform(x):\n",
    "    return np.array([1.0, 1.0], [-1.0, 1.0])\n",
    "\n",
    "def inv_transform(x):\n",
    "    return np.array([ .5 * (x[0] + x[1]), .5 * (x[0] - x[1])]) \n",
    "\n",
    "n_umap = umap.reparametrize(transform, der_transform, inv_transform, (2,), inherit_methods=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bae2862",
   "metadata": {},
   "source": [
    "Note that reparametrize by default implements methods such as kl and grad_kl using the methods from the object's methods, and not from the generic ProbaMap class. If this is not desirable, one should specify inherit_methods = False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9331a6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "tic = time()\n",
    "n_umap.kl([0,.5], [0, .6], 10**5)\n",
    "tac = time()\n",
    "print(f\"Time elapsed after reparametrization: {tac - tic}\")\n",
    "\n",
    "tic = time()\n",
    "umap.kl([-.5,.5], [-.6, .6], 10**5)\n",
    "tac = time()\n",
    "print(f\"Time elapsed optimized: {tac - tic}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba18b11",
   "metadata": {},
   "source": [
    "## Exponential families\n",
    "\n",
    "An important type of probability families are exponential families, whose log-densities share a common form as essentially a scalar product between some representation of $x$ and $\\theta$, i.e.\n",
    "\\begin{equation}\n",
    "\\log\\left(\\frac{d\\pi_{\\theta}}{d\\pi}(x)\\right) = \\langle \\theta, T(x)\\rangle - g(\\theta) + h(x)\n",
    "\\end{equation}\n",
    "\n",
    "It is possible to create an exponential family from the class ExponentialFamily, if the functions T, g and h are coded. Specific kl and grad_kl methods are implemented for this subclass, as well as reparametrize to protect these changes.\n",
    "\n",
    "We give here as an example the construction of gaussian distributions. Note that the ExponentialFamily uses the natural parameters of the exponential family, which might not be the most common ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab49f094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussians again as deriving from Exponential Family\n",
    "\n",
    "from math import pi\n",
    "\n",
    "dim = 1\n",
    "\n",
    "def get_m_sigma(theta):\n",
    "    theta = np.array(theta)\n",
    "    inv_sigma = -2 * theta[1:]\n",
    "    inv_sigma_m = theta[0]\n",
    "    sigma = np.linalg.inv(inv_sigma)\n",
    "    m = sigma @ inv_sigma_m\n",
    "    return m, sigma\n",
    "\n",
    "def gen(theta):\n",
    "    m, sigma = get_m_sigma(theta)\n",
    "    def loc_gen(n:int):\n",
    "        return np.random.multivariate_normal(mean = m, cov=sigma, size = n)\n",
    "    return loc_gen\n",
    "    \n",
    "def T(x):\n",
    "    accu = np.zeros((dim+1, dim))\n",
    "    accu[0] = x\n",
    "    accu[1:] = np.outer(x, x)\n",
    "    return accu\n",
    "\n",
    "def g(theta):\n",
    "    inv_sigma = -2 * theta[1:]\n",
    "    inv_sigma_m = theta[0]\n",
    "    mu, sigma = get_m_sigma(theta)\n",
    "    return .5 * (np.sum(mu * inv_sigma_m) + np.log(np.linalg.det(sigma)))\n",
    "\n",
    "def der_g(theta):\n",
    "    accu = np.zeros((dim+1,dim))\n",
    "    m, sigma = get_m_sigma(theta)\n",
    "    accu[0] = m\n",
    "    accu[1:] = sigma + np.outer(m, m)\n",
    "    return accu\n",
    "\n",
    "def der_der_g(theta):\n",
    "    # Using the fact that the distribution is one dimensional\n",
    "    H = np.array([\n",
    "        [-.5 * theta[1,0] ** (-1),\n",
    "         .5 * theta[0, 0] * (theta[1,0] ** (-2)) ],\n",
    "        [.5 * theta[0, 0] * (theta[1,0] ** (-2)), \n",
    "         -.5 * (theta[0,0] ** 2) * (theta[1,0] ** (-3)) + .5 * (theta[1,0] ** (-2))  ]])\n",
    "    return H.reshape((2,1,2,1))\n",
    "\n",
    "def h(x):\n",
    "    return - .5 * dim * np.log(2 * pi)\n",
    "\n",
    "ref_param = np.zeros((dim+1, dim))\n",
    "ref_param[1:] = -.5 * np.eye(dim)\n",
    "gmap = proba.ExponentialFamily(\n",
    "    gen, T, g, der_g, der_der_g, h,\n",
    "    distr_param_shape= (dim+1,dim), ref_param = ref_param, sample_shape=(dim,))\n",
    "\n",
    "plt.hist(gmap(gmap.ref_param)(100000)[:,0], 50, density=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98feddfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the kl/grad_kl computation is correct\n",
    "d_shape = gmap.distr_param_shape\n",
    "\n",
    "par1 = np.random.normal(0,1, d_shape) - np.array([[0], [3]])\n",
    "par2 = np.random.normal(0,1, d_shape) - np.array([[0], [2]])\n",
    "d_par = np.random.normal(0, 0.001, d_shape)\n",
    "grad_kl, kl = gmap.grad_right_kl(par1)(par2)\n",
    "(gmap.kl(par1, d_par + par2) - kl) / np.sum(grad_kl * d_par)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076f97e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is possible to recover the usual parametrisation\n",
    "def transform(x):\n",
    "    inter = x[1] ** (-2)\n",
    "    return np.array([[inter * x[0]], [-.5 * inter]])\n",
    "\n",
    "def der_transform(x):\n",
    "    inter = x[1] ** (-2)\n",
    "    return np.array([\n",
    "        [inter,                       0              ], \n",
    "        [- 2 * x[0] * (x[1] ** (-3)), (x[1] ** (-3)) ]]\n",
    "        ).reshape((2, 2,1))\n",
    "\n",
    "def inv_transform(x):\n",
    "    return np.array([x[0, 0]/ (-2 * x[1,0]), (- 2 * x[1,0]) ** (-1/2)])\n",
    "\n",
    "standard_gmap = gmap.reparametrize(transform, der_transform, inv_transform, distr_param_shape=(2,))\n",
    "\n",
    "# Check that the derivative is correct:\n",
    "x = np.random.normal(0,1,2)\n",
    "dx = np.random.normal(0, 10**-8, 2)\n",
    "\n",
    "err = np.max(np.abs((transform(x+dx) - transform(x) )/ np.tensordot(dx, der_transform(x), (0, 0)) -1))\n",
    "assert err < 10** -4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7a6682",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We can check that everything is working properly\n",
    "x = np.random.normal(0,1, (1000, 2))\n",
    "y = np.random.normal(0,1, (1000, 2))\n",
    "x[:, 1] = np.abs(x[:, 1])\n",
    "y[:, 1] = np.abs(y[:,1])\n",
    "\n",
    "res = np.log(np.abs(y[:,1]/x[:,1])) + (x[:,1] ** 2 + (x[:,0] - y[:,0]) **2 )/ (2 * (y[:,1] ** 2)) - .5\n",
    "kl_comp = standard_gmap.kl\n",
    "\n",
    "print(f\"Max error in 1000 kl computation: {np.max(np.array([kl_comp(mu1, mu2) for mu1, mu2 in zip(x,y)]) - res)}\")\n",
    "\n",
    "accu = np.zeros(100)\n",
    "for i in range(100):\n",
    "    x = np.random.normal(0,1, 2)\n",
    "    x[1] = np.abs(x[1])\n",
    "\n",
    "    y = np.random.normal(0,1, 2)\n",
    "    y[1] = .1 + np.abs(y[1])\n",
    "\n",
    "    y2 = y + np.random.normal(0, 10**(-6), 2)\n",
    "\n",
    "    grad_kl = standard_gmap.grad_kl(x)\n",
    "\n",
    "    accu[i] = (kl_comp(y2, x) - kl_comp(y, x)) / np.tensordot(grad_kl(y)[0],  (y2 - y), (0,0)) - 1\n",
    "print(f\"Max gradient estimation error in 100 computations: {np.max(np.abs(accu))}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa54019",
   "metadata": {},
   "source": [
    "### More on reparametrize\n",
    "\n",
    "The reparametrize method can also be used when considering a subclass of probability. In that setting, the inverse map is not used, and one should pass the new ref_param. We construct as an example the case the family of probabilities of form\n",
    "\n",
    "$$ \\sigma, \\mu \\rightarrow\\mathcal{N}(\\mu, \\sigma C)$$\n",
    "\n",
    "for $C$ a predefined matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b01ce2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing a covariance matrix\n",
    "dim = 2\n",
    "pre_cov = np.random.normal(0,1,(dim,dim))\n",
    "\n",
    "def transform(x):\n",
    "    \"\"\" To obtain class \\alpha, \\sigma ->  N(\\alpha, \\sigma C) for a fixed C\"\"\"\n",
    "    out = np.zeros((dim+1, dim))\n",
    "    out[0] = x[:-1]\n",
    "    out[1:] = x[-1] * pre_cov\n",
    "    return out\n",
    "\n",
    "def der_transform(x):\n",
    "    der_out = np.zeros((dim+1, dim+1, dim))\n",
    "    der_out[:-1, 0,:] = np.eye(dim)\n",
    "    der_out[-1, 1:] = pre_cov\n",
    "    return der_out\n",
    "\n",
    "# One can check that the derivative is functional\n",
    "x = np.random.normal(0,1, dim + 1)\n",
    "dx = np.random.normal(0,0.001, dim + 1)\n",
    "assert np.max(np.abs((transform(x+dx) - transform(x) )/ np.tensordot(dx, der_transform(x), (0, 0)) - 1)) < 10 ** (-5)\n",
    "\n",
    "# Construct the NewMap\n",
    "NewMap = proba.GaussianMap(dim).reparametrize(transform, der_transform, distr_param_shape= (dim+1, ))\n",
    "\n",
    "x = np.zeros(dim+1)\n",
    "x[-1] = 1.0\n",
    "print(NewMap(x))\n",
    "x[-1] = 2.0\n",
    "print(NewMap(x))\n",
    "\n",
    "x = np.random.normal(0,2,dim+1)\n",
    "x[-1] = 1.0\n",
    "distr0 = NewMap(x)\n",
    "sample0 = distr0(1000)\n",
    "\n",
    "# x[-1] = 2.0\n",
    "x = np.random.normal(0,2,dim+1)\n",
    "distr1 = NewMap(x)\n",
    "sample1 = distr1(1000)\n",
    "plt.plot(sample1[:,0], sample1[:,1], \",\", label=\"Second distribution\")\n",
    "plt.plot(sample0[:,0], sample0[:,1], \",\",  label=\"First distribution\")\n",
    "plt.legend()\n",
    "\n",
    "plt.title(\"Samples from 2 distributions with colinear covariance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18088d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KL and its gradient computation benefits from the original implementation\n",
    "grad, kl = NewMap.grad_kl(x)(2 * x)\n",
    "dx = np.random.normal(0,10**(-6), NewMap.distr_param_shape)\n",
    "kl_bis = NewMap.kl(2 * x + dx, x)\n",
    "(kl_bis - kl ) / (np.sum(dx * grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9437a79",
   "metadata": {},
   "source": [
    "# Probability tensorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e57e38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmap1 = proba.GaussianMap(3)\n",
    "gauss1 = gmap1(np.random.normal(0,1, gmap1.distr_param_shape)).reshape((1,3))\n",
    "gauss2 = gmap1(np.random.normal(0,1, gmap1.distr_param_shape)).reshape((1, 3))\n",
    "gauss3 = gmap1(np.random.normal(0,1, gmap1.distr_param_shape)).reshape((1, 3))\n",
    "\n",
    "distr = proba.tensorize(gauss1, gauss2, gauss3, flatten=False, dim = 0)\n",
    "\n",
    "y = np.random.normal(0,1, distr.sample_shape)\n",
    "assert distr.log_dens(y) == gauss1.log_dens(y[:1]) + gauss2.log_dens(y[1:2]) + gauss3.log_dens(y[2:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497291c9",
   "metadata": {},
   "source": [
    "### Combinations of ProbaMap objects\n",
    "So far, only tensorization of ProbaMaps objects is supported.\n",
    "\n",
    "The new parametrisation takes flat array which are split and reshaped in order. The resulting ProbaMap object benefits from the methods of the initial ProbaMap objects when computing kl and its gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9090eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proba Map\n",
    "gmap1 = proba.GaussianMap(3)\n",
    "gmap2 = proba.GaussianMap(2)\n",
    "\n",
    "gmap_tot = proba.map_tensorize(gmap1, gmap2)\n",
    "print(f\"The resulting ProbaMap takes flat inputs: {gmap_tot.distr_param_shape}\")\n",
    "\n",
    "par, par2 = np.random.normal(0,1, (2,) + gmap_tot.distr_param_shape)\n",
    "tic = time()\n",
    "gmap_tot.grad_kl(par)(par2)\n",
    "tac = time()\n",
    "print(f\"KL/Grad KL computation times should be efficcient: {tac - tic} s\")\n",
    "\n",
    "accu = []\n",
    "for _ in range(10):\n",
    "    param =  np.random.normal(0,1, 18)\n",
    "    grad_kl = gmap_tot.grad_kl(param)\n",
    "    for _ in range(100):\n",
    "        param1 = np.random.normal(0,1, 18)\n",
    "        delta_par = np.random.normal(0,10 ** -7, 18)\n",
    "\n",
    "        der_kl, kl = grad_kl(param1)\n",
    "        _, kl2 = grad_kl(param1 + delta_par)\n",
    "        accu.append((kl2 -kl) / np.sum(der_kl * delta_par))\n",
    "\n",
    "print(f\"Max KL gradient estimation error in 10 * 100 tries: {np.max(np.abs(np.log(accu)))}\")\n",
    "\n",
    "param = np.random.normal(0,1,18)\n",
    "\n",
    "gaussian_distr = gmap_tot(param)\n",
    "log_dens_der = gmap_tot.log_dens_der(param)\n",
    "\n",
    "delta_param = np.random.normal(0,10 ** -7, 18)\n",
    "\n",
    "d_gaussian_distr = gmap_tot(param + delta_param)\n",
    "sample = gaussian_distr(200)\n",
    "\n",
    "accu = []\n",
    "for y in sample:\n",
    "\n",
    "    delta_log = d_gaussian_distr.log_dens(y) - gaussian_distr.log_dens(y)\n",
    "\n",
    "    pred_delta_log = np.sum(log_dens_der(y) * delta_param)\n",
    "    accu.append(delta_log/pred_delta_log)\n",
    "\n",
    "print(f\"Max log-dens gradient estimation error in {len(sample)} trials: {np.max(np.abs(np.log(accu)))}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed875b1",
   "metadata": {},
   "source": [
    "## Implementation checks\n",
    "\n",
    "Implementations of sub family of ProbaMap should be thoroughly tested before use. To help that, functions in submodule _test can be used. These functions are designed to perform routine checks to see whether the grad_kl/grad_right_kl, log_dens_der and log_dens functions are coherent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c3c190",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aduq.proba.gauss import FixedCovGaussianMap, FactCovGaussianMap\n",
    "from aduq.proba.gauss.TGauss import TensorizedGaussianMap\n",
    "\n",
    "import aduq.proba._test as test\n",
    "tgm = TensorizedGaussianMap\n",
    "FiCGM = FixedCovGaussianMap\n",
    "FaCGM = FactCovGaussianMap\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "test.check_grad_kl( tgm(sample_shape=(2,)))\n",
    "test.check_grad_right_kl( tgm(sample_shape=(2,)))\n",
    "test.check_log_dens_der( tgm(sample_shape=(2,)))\n",
    "\n",
    "test.check_grad_kl( FiCGM(sample_shape=(2,)))\n",
    "test.check_grad_right_kl( FiCGM(sample_shape=(2,)))\n",
    "test.check_log_dens_der( FiCGM(sample_shape=(2,)))\n",
    "\n",
    "test.check_grad_kl( FaCGM(sample_shape=(2,)))\n",
    "test.check_grad_right_kl( FaCGM(sample_shape=(2,)))\n",
    "test.check_log_dens_der( FaCGM(sample_shape=(2,)), delta=10 ** -8, tol= 0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0b896b",
   "metadata": {},
   "source": [
    "## Uniform priors: the gaussian point of view\n",
    "\n",
    "In the following part, we exhibit how the ease of use of gaussian distributions could be combined with uniform priors requirement. For simplicity, we consider that the prior should be uniform on a hypercube $[0,1]^d$. Variants could be defined on hyper-rectangles or even on hyperspheres.\n",
    "\n",
    "The key idea is simple: if $X_1, \\dots, X_n$ are drawn i.i.d. from $N(0,1)$, then $(F^{-1}(X_1), \\dots, F^{-1}(X_n))$ is drawn uniformly on the hypercube (for hypershere prior, consider renormalisation of $X$ by appropriate quantity $f(\\lVert X\\rVert)$).\n",
    "\n",
    "We're ready to construct a family distributions on $[0,1]^d$: Consider $F^{-1}(N(\\mu, \\Sigma))$ where $F^{-1}$ is applied component wise. This mapping preserves groups independence. It is also increasing component-wise, so that it preserves stochastic dominance (for gaussian, stochastic dominanc partial order can only be assessed between distributions with same variance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7a683f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Gaussian to Uniform mapping.\n",
    "ghmap = proba.GaussHypercubeMap(2)\n",
    "\n",
    "x_axis_labels = np.linspace(10**-4,1 - 10** -4, 181)\n",
    "y_axis_labels = np.linspace(10**-4,1- 10 ** -4, 181)\n",
    "values = np.array(np.meshgrid(y_axis_labels, x_axis_labels)).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0f8007",
   "metadata": {},
   "source": [
    "The resulting distributions take values on the hypercube, and as such are good candidates for approximations of posterior. Reminiscent of beta distributions, the distributions can take different forms. If the covariance is smaller than identity, the distributions are unimodal. If this is not the case, the distribution can put more mass on some of the boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ab1f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = ghmap(np.array(\n",
    "    [[-0.3, 0.2],\n",
    "    [.1,-0.05],\n",
    "    [-0.05, .1]]))\n",
    "\n",
    "z = prob.dens(values)\n",
    "\n",
    "sns.heatmap(z, xticklabels=x_axis_labels, yticklabels=y_axis_labels)\n",
    "plt.title(\"Gaussian-ish behavior: localized distribution\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fa7cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = ghmap(np.array([[-.3,1.5], [.2,-0.1], [-0.1, .2]]))\n",
    "\n",
    "z = prob.dens(values)\n",
    "\n",
    "sns.heatmap(z, xticklabels=x_axis_labels, yticklabels=y_axis_labels)\n",
    "plt.title(\"Behavior on the border: localized curved distribution\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b00c609",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = ghmap([[0.0,0.0], [0.7,-0.3], [-0.3, .3]])\n",
    "\n",
    "z = prob.dens(values)\n",
    "\n",
    "sns.heatmap(z, xticklabels=x_axis_labels, yticklabels=y_axis_labels)\n",
    "plt.title(\"Large covariance span: S behavior\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7fc786",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = ghmap([[0.0,0.0], [1.02,0.0], [0.0, 1.02]])\n",
    "\n",
    "z = prob.dens(values)\n",
    "\n",
    "sns.heatmap(z, xticklabels=x_axis_labels, yticklabels=y_axis_labels)\n",
    "plt.title(\"Cushion distribution\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc3f6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = ghmap([[0.0,0.0], [1.01,0.0], [0.0, 0.99]])\n",
    "\n",
    "z = prob.dens(values)\n",
    "sns.heatmap(z, xticklabels=x_axis_labels, yticklabels=y_axis_labels)\n",
    "plt.title(\"Saddle distribution\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff7f2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = ghmap([\n",
    "    [0.0,0.0],\n",
    "    [1.0,0.0],\n",
    "    [0.2, 1.0]])\n",
    "\n",
    "z = prob.dens(values)\n",
    "sns.heatmap(z, xticklabels=x_axis_labels, yticklabels=y_axis_labels)\n",
    "plt.title(\"Saddle distribution\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6ed326",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aduq.proba.gauss.Gauss.gaussian_map import make_cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31ea55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_param(c):\n",
    "    vals, vects = np.linalg.eigh([[1.0, c], [c, 1.0]])\n",
    "    pre_cov =  (vals ** .5) * vects.T\n",
    "    out = np.zeros((3,2))\n",
    "    out[1:] = pre_cov\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2485d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = ghmap(make_param(-.7))\n",
    "\n",
    "x_axis_labels = np.linspace(.1, .9, 181) # Avoid renormalisation issue at the angles\n",
    "y_axis_labels = np.linspace(.1, .9, 181)\n",
    "values = np.array(np.meshgrid(y_axis_labels, x_axis_labels)).T\n",
    "\n",
    "z = prob.dens(values)\n",
    "sns.heatmap(z, xticklabels=x_axis_labels, yticklabels=y_axis_labels)\n",
    "plt.title(\"A Copula\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f41c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(prob(10**4)[:, 0], 40, density=True)\n",
    "plt.hist(prob(10**4)[:, 1], 40, density=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31490408",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
